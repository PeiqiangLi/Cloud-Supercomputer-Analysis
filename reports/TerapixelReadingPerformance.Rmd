---
title: "Performance evaluation of Terapixel rendering in Cloud (Super)computing"
author: "Peiqiang Li - 200987503"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,fig.align = 'center', out.width = "60%")
knitr::opts_knit$set(root.dir = '/Users/lipeiqiang/Documents/RWorks/Cloud_Computing')
```

```{r ProjectTemplate, include=FALSE}
library(ProjectTemplate)
load.project()
```

# Introduce

"Cloud computing refers to both the applications delivered as services over the Internet and the hardware and systems software in the data centers that provide those services"\cite{ref1}.Cloud computing concentrates computing and storage at the heart of the service, and high-bandwidth connections are adopted to link high-performance machines, so that all these resources could be carefully managed. The user sends a computing request to the cloud computing service and obtains the final result\cite{ref2}. In this project, Cloud (super)computing is used for high quality terapixel visualization. The visualization of trillions of pixels requires powerful computing resources. Therefore, cloud-based supercomputer resources provide greater flexibility for visualization.

# Methodology

The structure of the project is built by **ProjectTemplate**. As we all know, the correctness and reproducibility of data science code is very important. Thus, an appropriate project template can not only improve the efficiency of development, but also make the code and documents more concise and orderly.

Git is used as a version control tool for this project. All development activities are being recorded using Git and stored in a private repository. Due to the size and privacy of dataset, the raw data will not be uploaded to the GitHub repository.

Rstudio is the development tool of this project, The project process adopts the data exploratory analysis steps of **CRISP-DM** model. And, the literate programming framework is used to record the results of data analysis and generate the final project report.

README file will be used to record the analysis steps of the project and the reproduction process of the analysis.

# Business Understanding

[The realistic terapixel visualization of the city of Newcastle upon Tyne](http://terapixel.wasabi.1024.s3.eu-central-1.wasabisys.com/vtour/index.html) is calculated by cloud-based supercomputer resources. In addition, the rendered 3D visual city also supports daily updates, hence it is necessary to evaluate the scalable architecture of cloud supercomputer for optimal performance.



The project detail see [link] (https://github.com/NewcastleDataScience/StudentProjects202122/blob/master/TeraScope/Summary.md).

# Data Understanding

The dataset collected from the project link above. It contains the running information of ```r length(unique(gpu$hostname))`` 1024 GPU nodes. The running task is divided into three parts, which are used to render the visualization output of level 4, 8 and 12 respectively. In this dataset we can get detailed information about the performance timings of the render application, the performance of the GPU card, and which part of the image is being rendered in each task. The size of the dataset, the number of fields and the specific explanations are shown in the following tables:

```{r, include=FALSE}
dim(application.checkpoints)
```

The application.checkpoints dataset contains 660400 data and 6 fields.

\begin{table}[ht]
\centering
\begin{tabular}{c|c|c}
  \hline
 & Fields & Info \\ 
  \hline
1 & timestamp & Timestamp \\ 
  2 & hostname & Hostname of the virtual machine auto-assigned by the Azure batch system. \\ 
  3 & eventName & Name of the event occuring within the rendering application. \\ 
  4 & eventType &  Possible Values: "START", "STOP" \\ 
  5 & jobId &  ID of the Azure batch job. \\ 
  6 & taskId &  ID of the Azure batch task. \\ 
  \hline
\end{tabular}
\caption{dataset information of application.checkpoints} 
\label{tab4}
\end{table}

The explain of `eventName` values:

- **TotalRender** is the entire task.
- **Render** is when the image tile is is being rendered.
- **Saving Config** is simply a measure of configuration overhead.
- **Tiling** is where post processing of the rendered tile is taking place.
- **Uploading** is where the output from post processing is uploaded to Azure Blob Storage.

examples:

```{r}
head(application.checkpoints)
```

\newpage

```{r, include=FALSE}
dim(gpu)
```

The GPU dataset contains 1543681 data and 8 fields.

\begin{table}[ht]
\centering
\begin{tabular}{c|c|c}
  \hline
 & Fields & Info \\ 
  \hline
1 & timestamp & Timestamp \\ 
  2 & hostname & Hostname of the virtual machine auto-assigned by the Azure batch system. \\ 
  3 & gpuSerial & The serial number of the physical GPU card. \\ 
  4 & gpuUUID & The unique system id assigned by the Azure system to the GPU unit. \\ 
  5 & powerDrawWatt & Power draw of the GPU in watts. \\ 
  6 & gpuTempC & Temperature of the GPU in Celsius \\ 
  7 & gpuUtilPerc & Percent utilisation of the GPU Core(s). \\ 
  8 & gpuMemUtilPerc & Percent utilisation of the GPU memory. \\ 
   \hline
\end{tabular}
\caption{dataset information of gpu.csv} 
\label{tab4}
\end{table}

examples:

```{r}
head(gpu)
```

---

```{r, include=FALSE}
dim(task.x.y)
```

The task dataset contains 65793 data and 5 fields and the specific information of **task-x-y.csv**:

- **jobId**: Id of the Azure batch job.
- **taskId**: ID of the Azure batch task.
- **x**: X co-ordinate of the image tile being rendered.
- **y**: Y co-ordinate of the image tile being rendered.
- **level**: The visualisation created is a zoomable "google maps style" map. In total we create 12 levels. Level 1 is zoomed right out and level 12 is zoomed right in. You will only see levels 4, 8 and 12 in the data as the intermediate level are derived in the tiling process.

examples:

```{r}
head(task.x.y)
```

# Data Preparation

# Data Analysis

## The event types dominate task runtimes



```{r}
mean_task_time %>% mutate(eventType = fct_reorder(eventType,duration)) %>%
  ggplot(aes(x=eventType, y=duration)) +
    geom_bar(stat="identity",fill="#1380A1") +
    coord_flip() +
    theme_ipsum(base_family = "Helvetica") +
    theme(
      panel.grid.minor.y = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.grid.major.x = element_line(),
      panel.grid.minor.x = element_line(),
      plot.title = element_text(size=20),
      legend.position="none"
    ) +
    ggtitle("Event types occupy task runtimes(s)") +
    xlab("") +
    ylab("")
```

# Conclusoin



\newpage

\begin{thebibliography}{99}  
\bibitem{ref1}Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica, I. and Zaharia, M., 2010. A view of cloud computing. Communications of the ACM, 53(4), pp.50-58.
\bibitem{ref2}Hayes, B., 2008. Cloud computing.
\bibitem{ref3}Christensen, R., 2006. Log-linear models and logistic regression. Springer Science \& Business Media.
\bibitem{ref4}McLeod, A.I. and Xu, C., 2010. bestglm: Best subset GLM. URL http://CRAN. R-project. org/package= bestglm.
\bibitem{ref5}McNeish, D.M., 2015. Using lasso for predictor selection and to assuage overfitting: A method long overlooked in behavioral sciences. Multivariate Behavioral Research, 50(5), pp.471-484.
\end{thebibliography}
